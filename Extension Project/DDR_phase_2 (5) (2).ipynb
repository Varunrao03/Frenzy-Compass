{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Web-Scraping For Travel Triangle Website"
      ],
      "metadata": {
        "id": "aaSIfCdYKAHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import requests  # For sending HTTP requests\n",
        "from bs4 import BeautifulSoup  # For parsing HTML content\n",
        "import pandas as pd\n",
        "import re  # used to perform various operations such as matching, searching, and replacing patterns in strings.\n",
        "import requests_cache  # For caching HTTP requests\n",
        "\n",
        "# URL of the TravelTriangle page for restaurants in San Francisco\n",
        "URL = 'https://traveltriangle.com/blog/restaurants-in-san-francisco/'\n",
        "\n",
        "# Setting up caching\n",
        "requests_cache.install_cache('restaurant_cache', expire_after=432000)\n",
        "\n",
        "\n",
        "# In the below function scrape_traveltriangle_restaurants, we initially fetch\n",
        "# the webpage using requests.get() with a specified User-Agent to mimic a\n",
        "# browser request.If the request fails, it catches the error and prints a message,\n",
        "# returning an empty DataFrame. Further, if the request is successful, it parses\n",
        "# the HTML content with BeautifulSoup for further extraction of restaurant\n",
        "# details.Lastly, it initializes an empty list restaurants where the extracted\n",
        "# restaurant data will be stored.\n",
        "\n",
        "def scrape_traveltriangle_restaurants(limit=10):\n",
        "\n",
        "    try:\n",
        "        response = requests.get(URL, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "        response.raise_for_status()  # Check if the HTTP request was successful (status code 200)\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Failed to retrieve the webpage: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Parse HTML content with BeautifulSoup for efficient extraction\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Initialize an empty list to store restaurant data\n",
        "    restaurants = []\n",
        "\n",
        "\n",
        "# Optimization:\n",
        "# we have used the try-except for ensuring that the ensures that if there is\n",
        "# an error with one restaurant (e.g., missing address or description), it\n",
        "# doesn't interrupt the entire scraping process. Instead, the scraper moves\n",
        "# on to the next restaurant, maintaining efficiency and resilience. Extract\n",
        "# restaurant names and details\n",
        "# We have also used limit to ensures that the loop will only search through\n",
        "# the first three sibling <p> tags after the address, preventing unnecessary\n",
        "# parsing of additional HTML content.\n",
        "\n",
        "\n",
        "    # Extract restaurant names and details efficiently, limiting the result by the 'limit' parameter\n",
        "    restaurant_headings = soup.find_all('h3')[:limit]  # Only fetch the first 'limit' restaurant headings\n",
        "\n",
        "    # Loop through each restaurant heading to extract the details\n",
        "    for heading in restaurant_headings:\n",
        "        try:\n",
        "            # Extract restaurant name from the <h3> tag\n",
        "            name = heading.get_text(strip=True)\n",
        "            # Extract description from the next sibling <p> tag if available\n",
        "            description = heading.find_next_sibling('p')\n",
        "            description_text = description.get_text(strip=True) if description else 'No details available'\n",
        "            # Extract address from the next <p> tag after description\n",
        "            address = description.find_next_sibling('p') if description else None\n",
        "            address_text = address.get_text(strip=True) if address else 'NA'\n",
        "            opening_hours = []\n",
        "            if address:\n",
        "                for sibling in address.find_next_siblings('p', limit=3):\n",
        "                    text = sibling.get_text(strip=True)\n",
        "                    if ':' in text:\n",
        "                        opening_hours.append(text)\n",
        "\n",
        "            # Clean up and format the address\n",
        "            formatted_address = re.sub(r'\\s+', ' ', address_text).strip()\n",
        "\n",
        "            # Append the restaurant data to the list\n",
        "            restaurants.append({\n",
        "                'Restaurant Name': name,\n",
        "                'Description': formatted_address,\n",
        "                'Address': '\\n'.join(opening_hours) if opening_hours else 'NA'\n",
        "            })\n",
        "        except AttributeError as e:\n",
        "            # In case of missing data or structure changes, skip this entry and continue with the next\n",
        "            print(f\"Error processing restaurant entry: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Convert the list of dictionaries to a pandas DataFrame for easy handling\n",
        "    df = pd.DataFrame(restaurants)\n",
        "\n",
        "    # Return the resulting DataFrame\n",
        "    return df\n",
        "\n",
        "# Running the scraper with a limit of 17 restaurants\n",
        "df_restaurants = scrape_traveltriangle_restaurants(limit=17)\n",
        "\n",
        "# Display the resulting DataFrame if data is scraped successfully, else notify the user\n",
        "if not df_restaurants.empty:\n",
        "    print(\"Restaurants suggested based on scraping data from Traveltriangle:\",df_restaurants)\n",
        "else:\n",
        "    print(\"No restaurant data found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwpK5e67HMtz",
        "outputId": "5a8ae37f-b242-48fa-caf4-efa2b9d71d5d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Restaurants suggested based on scraping data from Traveltriangle:                                       Restaurant Name  \\\n",
            "0                                        1. Zuni Cafe   \n",
            "1           Looking To Book An International Holiday?   \n",
            "2                                      2. Cliff House   \n",
            "3                                   3. Foreign Cinema   \n",
            "4                                4. Swan Oyster Depot   \n",
            "5                               5. House Of Prime Rib   \n",
            "6   Planning your holiday but confused about where...   \n",
            "7                        6. Brenda’s French Soul Food   \n",
            "8                                 7. House Of Nanking   \n",
            "9                                       8. Del Popolo   \n",
            "10                           9. State Bird Provisions   \n",
            "11                            10. Liholiho Yacht Club   \n",
            "12                                    11. Petit Crenn   \n",
            "13                                           12. Benu   \n",
            "14                            13. Tartine Manufactory   \n",
            "15                                        14. Cotogna   \n",
            "16                               15. Sons & Daughters   \n",
            "\n",
            "                                          Description  \\\n",
            "0   Zuni Café in San Francisco, California has won...   \n",
            "1                                                  NA   \n",
            "2   This popular restaurant is located above the c...   \n",
            "3   This restaurant has excellent infrastructure, ...   \n",
            "4   If you are seafood lover, you can get everythi...   \n",
            "5   You will get astonished by the beautiful and e...   \n",
            "6                                                  NA   \n",
            "7   Brenda, the Chef Proprietor, established her o...   \n",
            "8   You can calm your craving for Chinese food in ...   \n",
            "9   Del Popolo offers Neapolitan style pizza in Sa...   \n",
            "10  Yes, State Bird Provisions is a good choice fo...   \n",
            "11  This place has beautiful interior and serves u...   \n",
            "12  If you have love Atelier Crenn, then you will ...   \n",
            "13  Address:22 Hawthorne St, San Francisco, CA 941...   \n",
            "14  Address:595 Alabama St, San Francisco, CA 9411...   \n",
            "15  If you are looking for some cool restaurants i...   \n",
            "16  Want to satiate your hunger with some mouth-wa...   \n",
            "\n",
            "                                              Address  \n",
            "0   Address:1658 Market St, San Francisco, CA 9410...  \n",
            "1                                                  NA  \n",
            "2   Address:1090 Point Lobos Ave, San Francisco, C...  \n",
            "3   Address:2534 Mission St, San Francisco, CA 941...  \n",
            "4   Address:1517 Polk St, San Francisco, CA 94109,...  \n",
            "5   Address:1906 Van Ness Ave, San Francisco, CA 9...  \n",
            "6                                                  NA  \n",
            "7   Address:652 Polk St, San Francisco, CA 94102, ...  \n",
            "8   Address:919 Kearny St, San Francisco, CA 94133...  \n",
            "9   Address:855 Bush St, San Francisco, CA 94108, ...  \n",
            "10  Address:1529 Fillmore St, San Francisco, CA 94...  \n",
            "11  Address:871 Sutter St, San Francisco, CA 94109...  \n",
            "12  Address:609 Hayes St, San Francisco, CA 94102,...  \n",
            "13  Address:595 Alabama St, San Francisco, CA 9411...  \n",
            "14  Suggested Read:San Francisco Shopping: 10 Plac...  \n",
            "15  Address:490 Pacific Ave, San Francisco, CA 941...  \n",
            "16  Address:708 Bush Street, Btw. Powell & Mason S...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Web-Scraping for Wikipedia Page"
      ],
      "metadata": {
        "id": "oM4Zi11EKKSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Set up caching\n",
        "requests_cache.install_cache('restaurant_cache', expire_after=432000)\n",
        "\n",
        "# In the function scrap_top_restaurants, we initially send an HTTP GET request\n",
        "# to the Wikipedia URL, to which the response.status_code checks if the HTTP\n",
        "# request was successful or has failed to retrieve data. Finally uses\n",
        "# BeautifulSoup,to parse the HTML content of the page.\n",
        "\n",
        "def scrape_top_restaurants(wikipedia_url):\n",
        "    # Send a request to the Wikipedia page\n",
        "    response = requests.get(wikipedia_url)\n",
        "    if response.status_code != 200:\n",
        "        print(\"Failed to retrieve the page\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Parse the HTML content\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# This section of the code is designed to gather restaurant details, such as\n",
        "# their name, cuisine, and address, from the rows of a table on the\n",
        "# Wikipedia page. It first looks for any tables marked as wikitable, which\n",
        "# are commonly used for structured data. Once it finds the relevant table,\n",
        "# the code skips over the header row and begins processing each data row.\n",
        "# For each row, it checks if there are enough columns (at least three) to\n",
        "# ensure there’s sufficient information. If the row contains the right\n",
        "# amount of data, it extracts the restaurant's name from the first column,\n",
        "# the cuisine from the second, and the address from the third. Each set of\n",
        "# extracted details is then added to a list, which will eventually be used\n",
        "# to create a structured dataset\n",
        "\n",
        "    # Find the first table (adjust if necessary based on the page structure)\n",
        "    tables = soup.find_all('table', {'class': 'wikitable'})\n",
        "    if not tables:\n",
        "        print(\"No tables found on the page.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    data = []\n",
        "    for table in tables:\n",
        "        rows = table.find_all('tr')[1:]  # Skip header row\n",
        "        for row in rows:\n",
        "            cols = row.find_all('td')\n",
        "            if len(cols) >= 3:  # Ensure there are enough columns\n",
        "                restaurant_name = cols[0].text.strip()  # First column: Restaurant Name\n",
        "                Cuisines = cols[1].text.strip()  # Second column: Address\n",
        "                Address = cols[2].text.strip()  # Third column: Michelin Star\n",
        "                data.append([restaurant_name, Cuisines, Address])\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(data, columns=[\"Restaurant Name\", \"Cuisines\", \"Address\"])\n",
        "    return df\n",
        "\n",
        "# Example usage\n",
        "wikipedia_url = \"https://en.wikipedia.org/wiki/List_of_Michelin-starred_restaurants_in_the_San_Francisco_Bay_Area_and_Northern_California\"  # Change URL as needed\n",
        "top_restaurants_df = scrape_top_restaurants(wikipedia_url)\n",
        "print(\"Top Restaurants suggested from scraping Wikipedia's page are:\")\n",
        "print(top_restaurants_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uceFzK53tx_q",
        "outputId": "255952e8-1188-4d51-b145-ac7bf576c669"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Restaurants suggested from scraping Wikipedia's page are:\n",
            "    Restaurant Name                 Cuisines  \\\n",
            "0           7 Adams              Californian   \n",
            "1        Acquerello                  Italian   \n",
            "2             Adega  Portuguese, Californian   \n",
            "3        AL's Place              Californian   \n",
            "4            Angler    Contemporary, Seafood   \n",
            "..              ...                      ...   \n",
            "177         Trevese             New American   \n",
            "178          Ubuntu               Vegetarian   \n",
            "179            Wako                 Japanese   \n",
            "180        Wakuriya                 Japanese   \n",
            "181            [18]                     [19]   \n",
            "\n",
            "                                Address  \n",
            "0             San Francisco – Japantown  \n",
            "1            San Francisco – Polk Gulch  \n",
            "2                              San Jose  \n",
            "3      San Francisco – Mission District  \n",
            "4    San Francisco – Financial District  \n",
            "..                                  ...  \n",
            "177                           Los Gatos  \n",
            "178                                Napa  \n",
            "179                   Richmond District  \n",
            "180                           San Mateo  \n",
            "181                                [20]  \n",
            "\n",
            "[182 rows x 3 columns]\n"
          ]
        }
      ]
    }
  ]
}